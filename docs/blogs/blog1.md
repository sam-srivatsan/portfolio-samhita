Lyel’s lecture inspired me to read about Joy Buolamwini’s work (listed in his presentation as “people to follow”). Joy is the founder of the Algorithmic Justice League and was a PhD student at the MIT Media Lab. Her work aligns closely with the value-centered design principles that we have been using in 6.1040 and with the AI-ethics intersection that feels omnipresent on campus!

I will spend a lot of time throughout my career trying to iterate and optimize solutions fast. Lyel’s talk and Joy’s AJL website reminded me that sometimes it is just as important to slow down and prioritize the direction of progress.

One AJL study is called “Who Audits the Auditors?” It asks how we enforce accountability for the algorithms (loosely, the AI) that we deploy to be used however different institutions see fit. After surveying 150+ auditors and industry leaders, Joy found that 82% of auditors want to make standards public and transparent. I agree! But how many people would a shift to more transparent audit standards truly positively impact?

A lot of people around me allude to “AI” as a hand-wavy dystopia. How many of these daily AI users, whether non-coders through ChatGPT or fellow coders using a tool like GitHub Copilot, would actually be invested in learning about the specific audit standards and protocols? Does reframing standards such that they can be more understandable to this wider audience detract from the thoroughness of audits? Or could it make audits more effective by simplifying and streamlining the expectations for AI that we deploy for use by so many people? Do strong ethical design requirements have to be complex?

I’m not sure what I want out of my career just yet. But I hope the solutions that I work on are truly that - solutions - and not just arbitrary tasks. That means that I can’t distance myself from ethics or impact just because I hope to do technical work. These things are pretty synonymous with rigorous scientific standards, and that is something that a good engineer should hold themself to. Joy’s work reminds me to ask these questions of my own work: Which stakeholders does my technology affect? Is the technology itself transparent? Are the standards that I’m holding it to transparent? Ultimately, these build up to the bigger principle of socially conscious technology that Lyel encouraged us to consider. If we stay accountable, we move in the direction of progress. That makes it a lot easier to answer “Does my technology do more good than harm?” with an emphatic yes.

